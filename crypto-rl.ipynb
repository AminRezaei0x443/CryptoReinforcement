{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd01f51b96cccc4dfa31409bdf749341c66a0b1bfefc543e31ec98ac61ca3a6c72a",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.l1 = nn.Linear(inputs, 48)\n",
    "        self.l2 = nn.Linear(48, 10)\n",
    "        self.l3 = nn.Linear(10 , outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        return self.l3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 20\n",
    "TARGET_UPDATE = 3\n",
    "\n",
    "inputs = 3\n",
    "n_actions = 3\n",
    "\n",
    "policy_net = DQN(inputs, n_actions).to(device)\n",
    "target_net = DQN(inputs, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoEnvironment:\n",
    "    def __init__(self, frame, clip=0.7):\n",
    "        self.frame = frame\n",
    "        self.clip = clip\n",
    "        self.index = 0\n",
    "        self.positions = []\n",
    "        self.lastPosition = {}\n",
    "        self.realizedProfit = 0\n",
    "        self.stopLoss = 0.2\n",
    "        self.positionReward = 0\n",
    "    \n",
    "    def getState(self):\n",
    "        return self.frame.iloc[self.index]\n",
    "    \n",
    "    def isEnd(self):\n",
    "        if self.realizedProfit <= (-1 * self.stopLoss):\n",
    "            return True\n",
    "        return self.index + 1 == int(len(self.frame) * self.clip)\n",
    "    \n",
    "    def count(self):\n",
    "        return int(len(self.frame) * self.clip)\n",
    "    \n",
    "    def act(self, action):\n",
    "        if action == 0:\n",
    "            # hold\n",
    "            if \"type\" in self.lastPosition and self.lastPosition[\"type\"] == \"Buy\":\n",
    "                current = self.getState()\n",
    "                self.index += 1\n",
    "                after = self.getState()\n",
    "                cP = current[\"Close\"]\n",
    "                nP = after[\"Close\"]\n",
    "                reward = (nP - cP) / self.lastPosition[\"price\"]\n",
    "                self.realizedProfit += reward\n",
    "                if reward > 0:\n",
    "                    reward *= 5\n",
    "                else:\n",
    "                    reward *= 1\n",
    "                return reward, self.isEnd()\n",
    "            else:\n",
    "                return 0, self.isEnd()\n",
    "        elif action == 1:\n",
    "            # buy if balance\n",
    "            if \"type\" in self.lastPosition and self.lastPosition[\"type\"] == \"Buy\":\n",
    "                return 0, self.isEnd()\n",
    "            else:\n",
    "                current = self.getState()\n",
    "                self.lastPosition = {\n",
    "                    \"type\": \"Buy\",\n",
    "                    \"price\": current[\"Close\"]\n",
    "                }\n",
    "                self.positions.append(self.lastPosition)\n",
    "                self.index += 1\n",
    "                after = self.getState()\n",
    "                cP = current[\"Close\"]\n",
    "                nP = after[\"Close\"]\n",
    "                reward = (nP - cP) / self.lastPosition[\"price\"]\n",
    "                self.realizedProfit += reward\n",
    "                if reward > 0:\n",
    "                    reward *= 5\n",
    "                else:\n",
    "                    reward *= 1\n",
    "                return reward, self.isEnd()\n",
    "        elif action == 2:\n",
    "            # sell if has\n",
    "            if \"type\" in self.lastPosition and self.lastPosition[\"type\"] == \"Sell\":\n",
    "                return 0, self.isEnd()\n",
    "            elif \"type\" in self.lastPosition and self.lastPosition[\"type\"] == \"Buy\":\n",
    "                current = self.getState()\n",
    "                buyPrice = self.lastPosition[\"price\"]\n",
    "                self.lastPosition = {\n",
    "                    \"type\": \"Sell\",\n",
    "                    \"price\": current[\"Close\"]\n",
    "                }\n",
    "                # reward = (current[\"Close\"] - buyPrice) / buyPrice\n",
    "                self.positions.append(self.lastPosition)\n",
    "                return 0, self.isEnd()\n",
    "        return 0, self.isEnd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"dotusdt-1m-20000.csv\")\n",
    "cols = [\"Close\",\"Volume\",\"Number-of-trades\"]\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_portion = 0.7\n",
    "# dataTrain = data[:int(len(data) * train_portion)]\n",
    "# dataTest = data[int(len(data) * train_portion):]\n",
    "dataTrain = data[16000:19000]\n",
    "dataTest = data[19000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 1/100 [00:09<15:41,  9.51s/it]epoch 0 profit: 0.001872643837767446 mean: 9.36321918883723e-05\n",
      "  2%|▏         | 2/100 [00:18<15:22,  9.41s/it]epoch 1 profit: -0.019741383797026814 mean: -0.0009870691898513407\n",
      "  3%|▎         | 3/100 [00:27<15:05,  9.33s/it]epoch 2 profit: 0.014682761151196787 mean: 0.0007341380575598393\n",
      "  4%|▍         | 4/100 [00:36<14:48,  9.26s/it]epoch 3 profit: 0.025149851987222114 mean: 0.0012574925993611057\n",
      "  5%|▌         | 5/100 [00:45<14:28,  9.15s/it]epoch 4 profit: 0.03932786543458307 mean: 0.0019663932717291534\n",
      "  6%|▌         | 6/100 [00:54<14:02,  8.96s/it]epoch 5 profit: 0.04040120301979643 mean: 0.0020200601509898215\n",
      "  7%|▋         | 7/100 [01:02<13:43,  8.85s/it]epoch 6 profit: 0.016875597151975633 mean: 0.0008437798575987816\n",
      "  8%|▊         | 8/100 [01:12<13:42,  8.94s/it]epoch 7 profit: 0.060197842603334654 mean: 0.0030098921301667326\n",
      "  9%|▉         | 9/100 [01:20<13:32,  8.93s/it]epoch 8 profit: 0.02259726443181978 mean: 0.001129863221590989\n",
      " 10%|█         | 10/100 [01:29<13:17,  8.87s/it]epoch 9 profit: 0.04391884008578265 mean: 0.0021959420042891326\n",
      " 11%|█         | 11/100 [01:38<13:09,  8.87s/it]epoch 10 profit: 0.010423196208299478 mean: 0.0005211598104149738\n",
      " 12%|█▏        | 12/100 [01:47<13:05,  8.93s/it]epoch 11 profit: 0.03811733802290934 mean: 0.001905866901145467\n",
      " 13%|█▎        | 13/100 [01:56<12:52,  8.88s/it]epoch 12 profit: 0.006618233168082219 mean: 0.000330911658404111\n",
      " 14%|█▍        | 14/100 [02:05<12:41,  8.86s/it]epoch 13 profit: 0.023439869249128956 mean: 0.0011719934624564478\n",
      " 15%|█▌        | 15/100 [02:14<12:31,  8.85s/it]epoch 14 profit: 0.02685037913938515 mean: 0.0013425189569692575\n",
      " 16%|█▌        | 16/100 [02:23<12:26,  8.88s/it]epoch 15 profit: 0.06059470701141754 mean: 0.003029735350570877\n",
      " 17%|█▋        | 17/100 [02:32<12:35,  9.11s/it]epoch 16 profit: 0.02484909505741266 mean: 0.001242454752870633\n",
      " 18%|█▊        | 18/100 [02:41<12:27,  9.12s/it]epoch 17 profit: 0.0036872382070610938 mean: 0.00018436191035305468\n",
      " 19%|█▉        | 19/100 [02:50<12:17,  9.10s/it]epoch 18 profit: 0.00136527222237351 mean: 6.82636111186755e-05\n",
      " 20%|██        | 20/100 [02:59<12:07,  9.10s/it]epoch 19 profit: 0.024971112722828804 mean: 0.0012485556361414402\n",
      " 21%|██        | 21/100 [03:08<11:53,  9.03s/it]epoch 20 profit: 0.010243391189219478 mean: 0.0005121695594609739\n",
      " 22%|██▏       | 22/100 [03:17<11:37,  8.94s/it]epoch 21 profit: 0.04519851848895882 mean: 0.002259925924447941\n",
      " 23%|██▎       | 23/100 [03:26<11:21,  8.85s/it]epoch 22 profit: 0.0247511290738799 mean: 0.001237556453693995\n",
      " 24%|██▍       | 24/100 [03:35<11:20,  8.95s/it]epoch 23 profit: 0.00935412932387563 mean: 0.0004677064661937815\n",
      " 25%|██▌       | 25/100 [03:44<11:06,  8.89s/it]epoch 24 profit: -0.029599890747778895 mean: -0.0014799945373889446\n",
      " 26%|██▌       | 26/100 [03:52<10:57,  8.89s/it]epoch 25 profit: 0.006893731263891417 mean: 0.0003446865631945708\n",
      " 27%|██▋       | 27/100 [04:01<10:49,  8.89s/it]epoch 26 profit: 0.00622961137221695 mean: 0.0003114805686108475\n",
      " 28%|██▊       | 28/100 [04:10<10:33,  8.80s/it]epoch 27 profit: 0.020359508639559442 mean: 0.001017975431977972\n",
      " 29%|██▉       | 29/100 [04:19<10:23,  8.78s/it]epoch 28 profit: 0.005212640421677335 mean: 0.00026063202108386676\n",
      " 30%|███       | 30/100 [04:27<10:12,  8.75s/it]epoch 29 profit: 0.03337968474823243 mean: 0.0016689842374116213\n",
      " 31%|███       | 31/100 [04:36<10:04,  8.76s/it]epoch 30 profit: 0.06739379974800172 mean: 0.003369689987400086\n",
      " 32%|███▏      | 32/100 [04:45<09:57,  8.79s/it]epoch 31 profit: 0.04292161078607336 mean: 0.002146080539303668\n",
      " 33%|███▎      | 33/100 [04:54<09:49,  8.80s/it]epoch 32 profit: 0.006035857302724918 mean: 0.0003017928651362459\n",
      " 34%|███▍      | 34/100 [05:03<09:41,  8.81s/it]epoch 33 profit: -0.0165564140176063 mean: -0.000827820700880315\n",
      " 35%|███▌      | 35/100 [05:11<09:30,  8.78s/it]epoch 34 profit: -0.00011658129303789011 mean: -5.829064651894505e-06\n",
      " 36%|███▌      | 36/100 [05:20<09:24,  8.82s/it]epoch 35 profit: 0.040388434971450705 mean: 0.002019421748572535\n",
      " 37%|███▋      | 37/100 [05:29<09:18,  8.86s/it]epoch 36 profit: 0.03112460696319896 mean: 0.001556230348159948\n",
      " 38%|███▊      | 38/100 [05:38<09:09,  8.86s/it]epoch 37 profit: 0.01772141055213574 mean: 0.000886070527606787\n",
      " 39%|███▉      | 39/100 [05:47<09:05,  8.94s/it]epoch 38 profit: 0.02540160890746785 mean: 0.0012700804453733925\n",
      " 40%|████      | 40/100 [05:56<08:54,  8.92s/it]epoch 39 profit: -0.016878564318992085 mean: -0.0008439282159496042\n",
      " 41%|████      | 41/100 [06:05<08:42,  8.85s/it]epoch 40 profit: 0.048870425741526446 mean: 0.0024435212870763225\n",
      " 42%|████▏     | 42/100 [06:13<08:30,  8.80s/it]epoch 41 profit: -0.0038408000145996254 mean: -0.00019204000072998127\n",
      " 43%|████▎     | 43/100 [06:22<08:23,  8.83s/it]epoch 42 profit: 0.02986041222681194 mean: 0.001493020611340597\n",
      " 44%|████▍     | 44/100 [06:31<08:18,  8.90s/it]epoch 43 profit: 0.04709485297888738 mean: 0.002354742648944369\n",
      " 45%|████▌     | 45/100 [06:40<08:07,  8.87s/it]epoch 44 profit: -0.006326094491076334 mean: -0.0003163047245538167\n",
      " 46%|████▌     | 46/100 [06:49<07:54,  8.79s/it]epoch 45 profit: 0.013037702021365439 mean: 0.000651885101068272\n",
      " 47%|████▋     | 47/100 [06:58<07:44,  8.76s/it]epoch 46 profit: -0.022843606798421876 mean: -0.001142180339921094\n",
      " 48%|████▊     | 48/100 [07:06<07:33,  8.73s/it]epoch 47 profit: 0.03690340389535577 mean: 0.0018451701947677884\n",
      " 49%|████▉     | 49/100 [07:15<07:29,  8.80s/it]epoch 48 profit: -0.003762849357852794 mean: -0.0001881424678926397\n",
      " 50%|█████     | 50/100 [07:24<07:17,  8.76s/it]epoch 49 profit: 0.014224860784939109 mean: 0.0007112430392469555\n",
      " 51%|█████     | 51/100 [07:32<07:07,  8.71s/it]epoch 50 profit: -0.014743301890922907 mean: -0.0007371650945461453\n",
      " 52%|█████▏    | 52/100 [07:41<06:59,  8.74s/it]epoch 51 profit: 0.00780882534903163 mean: 0.0003904412674515815\n",
      " 53%|█████▎    | 53/100 [07:50<06:52,  8.77s/it]epoch 52 profit: -0.0038643617365582503 mean: -0.00019321808682791253\n",
      " 54%|█████▍    | 54/100 [07:59<06:45,  8.81s/it]epoch 53 profit: 0.026559083882149052 mean: 0.0013279541941074527\n",
      " 55%|█████▌    | 55/100 [08:08<06:36,  8.82s/it]epoch 54 profit: 0.04864935882491812 mean: 0.0024324679412459057\n",
      " 56%|█████▌    | 56/100 [08:17<06:35,  8.98s/it]epoch 55 profit: 0.0020687025424503402 mean: 0.00010343512712251701\n",
      " 57%|█████▋    | 57/100 [08:26<06:24,  8.95s/it]epoch 56 profit: -0.013683996734018176 mean: -0.0006841998367009088\n",
      " 58%|█████▊    | 58/100 [08:35<06:13,  8.88s/it]epoch 57 profit: 0.018436655544507456 mean: 0.0009218327772253729\n",
      " 59%|█████▉    | 59/100 [08:44<06:07,  8.96s/it]epoch 58 profit: 0.010942716118531962 mean: 0.000547135805926598\n",
      " 60%|██████    | 60/100 [08:53<06:01,  9.05s/it]epoch 59 profit: 0.005343859987364326 mean: 0.0002671929993682163\n",
      " 61%|██████    | 61/100 [09:02<05:48,  8.94s/it]epoch 60 profit: 0.01825596074368447 mean: 0.0009127980371842235\n",
      " 62%|██████▏   | 62/100 [09:11<05:37,  8.89s/it]epoch 61 profit: -0.0342150691463006 mean: -0.0017107534573150298\n",
      " 63%|██████▎   | 63/100 [09:19<05:28,  8.88s/it]epoch 62 profit: 0.02976988668721402 mean: 0.001488494334360701\n",
      " 64%|██████▍   | 64/100 [09:29<05:24,  9.01s/it]epoch 63 profit: 0.04758547838636783 mean: 0.0023792739193183913\n",
      " 65%|██████▌   | 65/100 [09:38<05:21,  9.19s/it]epoch 64 profit: -0.023585074238473412 mean: -0.0011792537119236705\n",
      " 66%|██████▌   | 66/100 [09:48<05:13,  9.23s/it]epoch 65 profit: 0.0006757647794165614 mean: 3.3788238970828066e-05\n",
      " 67%|██████▋   | 67/100 [09:59<05:21,  9.74s/it]epoch 66 profit: 0.018463791424528808 mean: 0.0009231895712264404\n",
      " 68%|██████▊   | 68/100 [10:08<05:05,  9.53s/it]epoch 67 profit: 0.021573211117889526 mean: 0.0010786605558944762\n",
      " 69%|██████▉   | 69/100 [10:17<04:50,  9.37s/it]epoch 68 profit: 0.026820698160582697 mean: 0.0013410349080291348\n",
      " 70%|███████   | 70/100 [10:26<04:37,  9.24s/it]epoch 69 profit: 0.03943726803619654 mean: 0.001971863401809827\n",
      " 71%|███████   | 71/100 [10:35<04:26,  9.18s/it]epoch 70 profit: 0.0417846716232311 mean: 0.002089233581161555\n",
      " 72%|███████▏  | 72/100 [10:44<04:16,  9.16s/it]epoch 71 profit: 0.032018303067085 mean: 0.0016009151533542502\n",
      " 73%|███████▎  | 73/100 [10:53<04:05,  9.11s/it]epoch 72 profit: -0.0045375735141384215 mean: -0.00022687867570692107\n",
      " 74%|███████▍  | 74/100 [11:02<03:59,  9.23s/it]epoch 73 profit: 0.014659720363771833 mean: 0.0007329860181885916\n",
      " 75%|███████▌  | 75/100 [11:12<03:51,  9.25s/it]epoch 74 profit: 0.011570882396328641 mean: 0.000578544119816432\n",
      " 76%|███████▌  | 76/100 [11:20<03:38,  9.11s/it]epoch 75 profit: 0.011712543843488474 mean: 0.0005856271921744237\n",
      " 77%|███████▋  | 77/100 [11:29<03:28,  9.09s/it]epoch 76 profit: -0.004836926521031327 mean: -0.00024184632605156632\n",
      " 78%|███████▊  | 78/100 [11:38<03:17,  8.97s/it]epoch 77 profit: 0.019177156268873976 mean: 0.0009588578134436988\n",
      " 79%|███████▉  | 79/100 [11:47<03:07,  8.95s/it]epoch 78 profit: 0.0688484284132737 mean: 0.0034424214206636846\n",
      " 80%|████████  | 80/100 [11:56<02:58,  8.94s/it]epoch 79 profit: -0.006238575211669418 mean: -0.0003119287605834709\n",
      " 81%|████████  | 81/100 [12:05<02:48,  8.87s/it]epoch 80 profit: 0.02130905043998127 mean: 0.0010654525219990635\n",
      " 82%|████████▏ | 82/100 [12:13<02:39,  8.86s/it]epoch 81 profit: 0.011699187591929953 mean: 0.0005849593795964976\n",
      " 83%|████████▎ | 83/100 [12:22<02:30,  8.85s/it]epoch 82 profit: 0.03303256569228244 mean: 0.001651628284614122\n",
      " 84%|████████▍ | 84/100 [12:31<02:22,  8.90s/it]epoch 83 profit: 0.0005898755261988546 mean: 2.9493776309942728e-05\n",
      " 85%|████████▌ | 85/100 [12:40<02:13,  8.91s/it]epoch 84 profit: 0.05482155629141265 mean: 0.0027410778145706325\n",
      " 86%|████████▌ | 86/100 [12:49<02:04,  8.86s/it]epoch 85 profit: 0.015930163589752595 mean: 0.0007965081794876298\n",
      " 87%|████████▋ | 87/100 [12:58<01:56,  8.97s/it]epoch 86 profit: 0.048017150286707624 mean: 0.0024008575143353813\n",
      " 88%|████████▊ | 88/100 [13:07<01:48,  9.02s/it]epoch 87 profit: 0.010624685153019937 mean: 0.0005312342576509969\n",
      " 89%|████████▉ | 89/100 [13:16<01:39,  9.06s/it]epoch 88 profit: 0.017160288283132152 mean: 0.0008580144141566076\n",
      " 90%|█████████ | 90/100 [13:26<01:30,  9.08s/it]epoch 89 profit: 0.009072467047301667 mean: 0.00045362335236508335\n",
      " 91%|█████████ | 91/100 [13:35<01:21,  9.05s/it]epoch 90 profit: -0.005935113651165104 mean: -0.0002967556825582552\n",
      " 92%|█████████▏| 92/100 [13:43<01:11,  8.98s/it]epoch 91 profit: -0.005242555525273051 mean: -0.00026212777626365255\n",
      " 93%|█████████▎| 93/100 [13:52<01:02,  8.91s/it]epoch 92 profit: 0.025307377262579433 mean: 0.0012653688631289717\n",
      " 94%|█████████▍| 94/100 [14:01<00:53,  8.94s/it]epoch 93 profit: -0.0038724144931709687 mean: -0.00019362072465854842\n",
      " 95%|█████████▌| 95/100 [14:10<00:45,  9.03s/it]epoch 94 profit: 0.053876760914549314 mean: 0.0026938380457274657\n",
      " 96%|█████████▌| 96/100 [14:19<00:35,  9.00s/it]epoch 95 profit: -0.010258477595818823 mean: -0.0005129238797909411\n",
      " 97%|█████████▋| 97/100 [14:29<00:27,  9.20s/it]epoch 96 profit: 0.010269858692459725 mean: 0.0005134929346229863\n",
      " 98%|█████████▊| 98/100 [14:38<00:18,  9.16s/it]epoch 97 profit: 0.025981263826317696 mean: 0.0012990631913158848\n",
      " 99%|█████████▉| 99/100 [14:47<00:09,  9.08s/it]epoch 98 profit: 0.032187930106001274 mean: 0.0016093965053000636\n",
      "100%|██████████| 100/100 [14:56<00:00,  8.96s/it]epoch 99 profit: 0.033197589786968774 mean: 0.0016598794893484387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 20\n",
    "dfList = np.array_split(dataTrain, num_episodes)\n",
    "\n",
    "def s2t(st):\n",
    "    return torch.tensor(st.to_list()).to(device).reshape(1, -1).float()\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    np.random.shuffle(dfList)\n",
    "    profit = 0\n",
    "    for i_episode in range(num_episodes):\n",
    "        env = CryptoEnvironment(dfList[i_episode], clip=1)\n",
    "        state =  s2t(env.getState())\n",
    "        for t in range(env.count()):\n",
    "            action = select_action(state)\n",
    "            reward, done = env.act(action.item())\n",
    "            reward = torch.tensor([reward], device=device).float()\n",
    "\n",
    "            if not done:\n",
    "                next_state = s2t(env.getState())\n",
    "            else:\n",
    "                next_state = None\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            optimize_model()\n",
    "            if done:\n",
    "                break\n",
    "        profit += env.realizedProfit\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    print(\"epoch {} profit: {} mean: {}\".format(epoch, profit, profit/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1522.55it/s]\n"
     ]
    }
   ],
   "source": [
    "def select_action2(state):\n",
    "    with torch.no_grad():\n",
    "        return target_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "evalEnv = CryptoEnvironment(dataTest, clip=1)\n",
    "state = s2t(evalEnv.getState())\n",
    "acts = []\n",
    "for t in tqdm(range(evalEnv.count())):\n",
    "    action = select_action2(state)\n",
    "    acts.append(action.item())\n",
    "    reward, done = evalEnv.act(action.item())\n",
    "    if not done:\n",
    "        next_state = s2t(evalEnv.getState())\n",
    "    else:\n",
    "        next_state = None\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  }
 ]
}